# -*- coding: utf-8 -*-
"""baseline_resnet50_no_pretrained.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v0kCNzW6fFXZnjOfRd0Zbu-tsh1ULqhr
"""

# ✅ 导入相关库
import os, json, random, zipfile
from pathlib import Path

import numpy as np
from PIL import Image
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from google.colab import drive

# ✅ Colab 文件路径与挂载配置
GOOGLE_DRIVE_FILE_PATH = "/content/drive/MyDrive/Colab Notebooks/PACS.zip"
COLAB_ZIP_NAME = "PACS.zip"
BASE_PACS_PATH = "kfold"

# ✅ 挂载 Google Drive
print("\n✅ 挂载 Google Drive")
drive.mount('/content/drive', force_remount=True)

# ✅ 解压 PACS 数据集
os.system(f"cp '{GOOGLE_DRIVE_FILE_PATH}' {COLAB_ZIP_NAME}")
with zipfile.ZipFile(COLAB_ZIP_NAME, 'r') as zip_ref:
    zip_ref.extractall(".")
print(f"Dataset base path is configured as: '{BASE_PACS_PATH}'")

# ✅ wandb 配置
USE_WANDB = True
if USE_WANDB:
    import wandb

# ✅ 设备选择
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

# ✅ 随机种子设置
def set_global_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if DEVICE.type == "cuda":
        torch.cuda.manual_seed_all(seed)

def worker_init_fn(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

def get_torch_generator(seed: int):
    g = torch.Generator()
    g.manual_seed(seed)
    return g

# ✅ PACS 数据集配置
PACS_DOMAINS = ["art_painting", "cartoon", "photo", "sketch"]
PACS_CATEGORIES = ["dog", "elephant", "giraffe", "guitar", "horse", "house", "person"]

# ✅ 训练超参数
MODEL_CHOICE = "resnet50"
PRETRAINED = False
NUM_EPOCHS = 40
BATCH_SIZE = 32
LEARNING_RATE = 1e-3
PATIENCE = 5
SEEDS = [42, 2024, 123]

# ✅ 模型与日志存储路径
SAVE_ROOT = "/content/drive/MyDrive/xAi_dg/baseline_unpretrained50_results/models"
Path(SAVE_ROOT).mkdir(parents=True, exist_ok=True)

# ✅ 数据集类
class PACS_Dataset(Dataset):
    def __init__(self, base_path, domain_name, categories, transform=None):
        self.transform = transform
        self.image_paths, self.labels = [], []
        for idx, cat in enumerate(categories):
            cat_path = Path(base_path) / domain_name / cat
            if not cat_path.is_dir(): continue
            for img_file in cat_path.iterdir():
                if img_file.suffix.lower() in {".jpg", ".jpeg", ".png"}:
                    self.image_paths.append(str(img_file))
                    self.labels.append(idx)

    def __len__(self): return len(self.image_paths)

    def __getitem__(self, idx):
        img = Image.open(self.image_paths[idx]).convert("RGB")
        if self.transform: img = self.transform(img)
        return img, self.labels[idx]

class SimpleImageDataset(Dataset):
    def __init__(self, paths, labels, transform=None):
        self.paths, self.labels, self.transform = paths, labels, transform
    def __len__(self): return len(self.paths)
    def __getitem__(self, idx):
        img = Image.open(self.paths[idx]).convert("RGB")
        if self.transform: img = self.transform(img)
        return img, self.labels[idx]

# ✅ 数据增强
mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
])
val_test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
])

# ✅ ResNet50 模型定义
def get_model(num_classes=len(PACS_CATEGORIES), pretrained: bool = False):
    weights = "IMAGENET1K_V1" if pretrained else None
    model = models.resnet50(weights=weights)
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    return model

# ✅ 单个 epoch 训练和验证逻辑
def train_one_epoch(model, loader, criterion, optimizer):
    model.train()
    running_loss, correct, total = 0.0, 0, 0
    for x, y in loader:
        x, y = x.to(DEVICE), y.to(DEVICE)
        optimizer.zero_grad()
        outputs = model(x)
        loss = criterion(outputs, y)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * x.size(0)
        correct += (outputs.argmax(1) == y).sum().item()
        total += y.size(0)
    return running_loss / total, correct / total

def eval_one_epoch(model, loader, criterion):
    model.eval()
    running_loss, correct, total = 0.0, 0, 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            outputs = model(x)
            loss = criterion(outputs, y)
            running_loss += loss.item() * x.size(0)
            correct += (outputs.argmax(1) == y).sum().item()
            total += y.size(0)
    return running_loss / total, correct / total

# ✅ 主训练流程
if __name__ == "__main__":
    all_seed_results = []

    for seed in SEEDS:
        set_global_seed(seed)
        seed_results = []

        for target_domain in PACS_DOMAINS:
            print(f"\n=== Seed {seed} | Target Domain: {target_domain} ===")
            source_domains = [d for d in PACS_DOMAINS if d != target_domain]

            # ✅ 全局 class 分层切分数据集（global split）
            domain_paths, domain_labels = [], []
            for d in source_domains:
                ds = PACS_Dataset(BASE_PACS_PATH, d, PACS_CATEGORIES)
                domain_paths.extend(ds.image_paths)
                domain_labels.extend(ds.labels)

            train_paths, val_paths, train_labels, val_labels = train_test_split(
                domain_paths,
                domain_labels,
                test_size=0.15,
                random_state=seed,
                stratify=domain_labels
            )

            g = get_torch_generator(seed)
            train_dataset = SimpleImageDataset(train_paths, train_labels, train_transform)
            val_dataset = SimpleImageDataset(val_paths, val_labels, val_test_transform)
            test_dataset = PACS_Dataset(BASE_PACS_PATH, target_domain, PACS_CATEGORIES, val_test_transform)

            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, worker_init_fn=worker_init_fn, generator=g)
            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)
            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

            run_prefix = f"seed{seed}_target_{target_domain}"
            log_path = Path(SAVE_ROOT) / f"log_{run_prefix}.json"
            checkpoint_path = Path(SAVE_ROOT) / f"checkpoint_{run_prefix}.pth"
            best_model_path = Path(SAVE_ROOT) / f"best_model_{run_prefix}.pth"

            # ✅ Resume 或重新训练逻辑
            if log_path.exists():
                with open(log_path) as f:
                    log_data = json.load(f)
                finished = log_data.get("finished", False)
            else:
                log_data, finished = None, False

            if finished and best_model_path.exists():
                print(f"✅ Finished log & best model found → skip training, load test acc from best_model.")
                model = get_model().to(DEVICE)
                model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))
                _, best_test_acc = eval_one_epoch(model, test_loader, nn.CrossEntropyLoss())
                seed_results.append(best_test_acc)
                continue

            if checkpoint_path.exists():
                print(f"🔄 Checkpoint found → resume training from checkpoint.")
                checkpoint = torch.load(checkpoint_path, map_location=DEVICE)
                model = get_model().to(DEVICE)
                model.load_state_dict(checkpoint["model_state_dict"])
                optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
                optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
                start_epoch = checkpoint["epoch"] + 1
                best_val_acc = checkpoint["best_val_acc"]
                patience_counter = checkpoint.get("patience_counter", 0)
                epoch_log = checkpoint.get("epoch_log", [])
            else:
                print(f"🟢 No valid checkpoint/log found → start training from scratch.")
                model = get_model(pretrained=PRETRAINED).to(DEVICE)
                optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
                start_epoch, best_val_acc, patience_counter, epoch_log = 1, 0.0, 0, []

            if USE_WANDB:
                wandb_run = wandb.init(
                    project="PACS_ResNet18_Baseline",
                    name=f"Seed{seed}_Src2Tar_{'+'.join(source_domains)}→{target_domain}",
                    config={"epochs": NUM_EPOCHS, "batch_size": BATCH_SIZE, "learning_rate": LEARNING_RATE, "model": MODEL_CHOICE, "seed": seed, "target_domain": target_domain},
                    reinit=True,
                )

            criterion = nn.CrossEntropyLoss()

            # ✅ 训练主循环
            for epoch in range(start_epoch, NUM_EPOCHS + 1):
                train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)
                val_loss, val_acc = eval_one_epoch(model, val_loader, criterion)
                print(f"Epoch {epoch}/{NUM_EPOCHS} train_acc={train_acc:.3f} val_acc={val_acc:.3f}")

                epoch_log.append({"epoch": epoch, "train_loss": train_loss, "train_acc": train_acc, "val_loss": val_loss, "val_acc": val_acc})
                if USE_WANDB:
                    wandb.log({"epoch": epoch, "train/loss": train_loss, "train/acc": train_acc, "val/loss": val_loss, "val/acc": val_acc})

                if val_acc > best_val_acc:
                    best_val_acc, patience_counter = val_acc, 0
                    torch.save(model.state_dict(), best_model_path)
                else:
                    patience_counter += 1

                torch.save({"epoch": epoch, "model_state_dict": model.state_dict(), "optimizer_state_dict": optimizer.state_dict(), "best_val_acc": best_val_acc, "patience_counter": patience_counter, "epoch_log": epoch_log}, checkpoint_path)

                if patience_counter >= PATIENCE:
                    print("Early stopping triggered!")
                    break

            # ✅ 最终测试评估
            model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))
            test_loss, test_acc = eval_one_epoch(model, test_loader, criterion)
            print(f"🎯 Final target-domain ({target_domain}) accuracy: {test_acc:.3f}")

            log_data = {"seed": seed, "target_domain": target_domain, "epochs": epoch_log, "best_val_acc": best_val_acc, "best_test_acc": test_acc, "finished": True}
            with open(log_path, "w") as f:
                json.dump(log_data, f, indent=2)

            seed_results.append(test_acc)
            if USE_WANDB:
                try:
                    wandb_run.summary["best_val_acc"] = best_val_acc
                    wandb_run.summary["test_acc"] = test_acc
                    wandb.finish()
                except Exception as e:
                    print(f"⚠️ WandB finish failed: {e}")

        all_seed_results.append(seed_results)

    # ✅ 结果统计
    mean_accuracies = np.mean(all_seed_results, axis=0).tolist()
    final_results = {"seeds": SEEDS, "domains": PACS_DOMAINS, "accuracies_per_seed_domain": all_seed_results, "mean_accuracies": mean_accuracies, "average_overall": float(np.mean(mean_accuracies)), "worst_case_overall": float(np.min(mean_accuracies))}
    with open(Path(SAVE_ROOT) / "baseline_results_multi_seed.json", "w") as f:
        json.dump(final_results, f, indent=2)
    print("\n✅ Training finished for all seeds! Results saved.")